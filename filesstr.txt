  Emotion-Aware Study Companion

   Project structure

emotion_aware_companion/
│
├── main.py
├── camera.py
├── audio_monitor.py
├── speech_input.py
├── affective_state.py
├── llm_agent.py
│
├── requirements.txt
└── README.md

   How it works (high level)

The system runs locally and continuously monitors facial and vocal cues.

-  Facial channel (implicit):  webcam frames are processed on-device to estimate emotion signals.
-  Vocal channel (implicit):  microphone input is monitored on-device to estimate fatigue / voice activity.

When the user explicitly interacts (pressing Enter and speaking), the system:

1.  Transcribes  the user’s speech into text (self-reported input).
2.  Summarizes  the recent affective state (valence, arousal, voice fatigue ratio).
3.  Fuses  implicit affective signals with the user’s explicit intent (transcript).
4. Sends only this  text + affective summary  to the LLM to generate  adaptive study recommendations .

> No raw audio or video is sent to the LLM—only the transcript and the affective summary.

   What this demonstrates

-  Affective computing understanding:  emotion is inferred from measurable cues and summarized as interpretable features (e.g., valence/arousal).
-  Human-in-the-loop:  the user’s explicit speech acts as the controlling input; the model supports, it does not decide in isolation.
-  Multimodal fusion:  combines facial + vocal signals with user text to build a more robust context.
-  Correct LLM usage (not a gimmick):  the LLM is used for *reasoning and recommendation generation* based on structured signals, not for emotion detection itself.
